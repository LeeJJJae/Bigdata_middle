{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc03ccc-4c2c-4c3a-8598-8de27efebdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder.master(\"local\").appName(\"midterm\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f201fa5-6a36-40fa-8fe8-c5fecd71df60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mezes Park'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_bd = spark_session.sparkContext.broadcast({'83':'Mezes Park', '84':'Ryland Park'})\n",
    "stations_bd.value[\"83\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9cd674-70f0-45ae-8f35-5836dd97c6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mezes Park'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationsfile = './data/stations.csv'\n",
    "stationsdata = dict(map(lambda x:(x[0], x[1]), map(lambda x: x.split(','), open(stationsfile))))\n",
    "stationsdata\n",
    "stations_bd = spark_session.sparkContext.broadcast(stationsdata)\n",
    "stations_bd.value[\"83\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a0a993-f56e-493a-9498-68c1ee78bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_bd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71ac5b8-89af-43ef-bcfc-99eb8927d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = spark_session.sparkContext.textFile(\"./data/status.csv\")\n",
    "stations = spark_session.sparkContext.textFile(\"./data/stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283f8cd6-d3cb-4eea-aed7-ab4569acf409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status1 = status.map(lambda x: x.split(',')).keyBy(lambda x: x[0])\n",
    "stations1 = stations.map(lambda x: x.split(',')).keyBy(lambda x: x[0])\n",
    "status1.join(stations1) \\\n",
    "    .map(lambda x:(x[1][0][3], x[1][1][1], x[1][0][1], x[1][0][2])) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4db5be-9cd3-4885-b625-542e8f610349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status1.map(lambda x:(x[1][3], stationsdata[x[0]], x[1][1], x[1][2]))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e50cbe5-7756-4492-9aa7-7f52e94da897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mezes Park'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_bd = spark_session.sparkContext.broadcast(stationsdata)\n",
    "stations_bd.value[\"83\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f03913-829b-49a6-9b3b-76d74f9decaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "907200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status1.map(lambda x:(x[1][3], stations_bd.value[x[0]], x[1][1], x[1][2]))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d06100-0d12-402b-afdd-2509fc4d5541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=spark_session.sparkContext.accumulator(0)\n",
    "def addone(x):\n",
    "    global acc\n",
    "    acc +=1\n",
    "    return x+1\n",
    "myrdd = spark_session.sparkContext.parallelize([1,2,3,4,5])\n",
    "myrdd.map(lambda x: addone(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284fd74f-dcc6-4bce-a34b-f58587ce34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records processed: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"records processed: \" + str(acc.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181b326e-6015-4c69-847c-1464d193bd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.core.rdd.Partitioner object at 0x7490822216d0>\n",
      "Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n"
     ]
    }
   ],
   "source": [
    "nums = range(0, 10) # nums = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "# partitionBy는 pair RDD에서만 의미\n",
    "rdd = spark_session.sparkContext.parallelize(nums).map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5471abda-aac8-4a49-8cc8-2868d729cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 3\n",
      "Partitioner: <pyspark.core.rdd.Partitioner object at 0x74914c36ee40>\n",
      "Partitions structure: [[('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Hungary', {'name': 'Johannes', 'amount': 200, 'country': 'Hungary'}), ('Hungary', {'name': 'Holland', 'amount': 400, 'country': 'Hungary'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})], [], [('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "def country_partitioner(country):\n",
    "    return portable_hash(country)\n",
    "\n",
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Hungary'},\n",
    "    {'name': 'Holland', 'amount': 400, 'country': 'Hungary'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "]\n",
    "rdd = spark_session.sparkContext.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(3, country_partitioner)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c947014a-773f-4e02-a932-b9099c7bd54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Hungary', {'name': 'Johannes', 'amount': 200, 'country': 'Hungary'}), ('Hungary', {'name': 'Holland', 'amount': 400, 'country': 'Hungary'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})]]\n"
     ]
    }
   ],
   "source": [
    "re_rdd = rdd.repartition(2)\n",
    "print(\"Number of partitions: {}\".format(re_rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(re_rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(re_rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af7828-fb58-494c-aa6f-fc0d4f2306f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
